---
id: NodeEditor
title: Node Editor
displayed_sidebar: webUiSidebar
---

import {
  EditOutlined
} from '@ant-design/icons';

## Node Overview

_Nodes_ are customizable blocks of code with default functionality specified by its type.

<div class="text--center">
<img width="300" alt="image" src="https://ganymede-bio.mo.cloudinary.net/apiServer/EditNodeBefore_20230522.png"/>
</div>

The image above shows how _nodes_ are represented on the Flow Editor page; each _node_ consists of:

- **Node Name**: located in the header of the _node_; Read_CSV in the image above.
- **Pencil Icon**: if present, the <EditOutlined className="button darkblue_button"/> is a button that opens the user-editable notebook when clicked.
- **Node Attribute(s)**: if present, the attributes associated with the _node_ that are processed by the node.  In the image above, the node attributes 'csv' and 'results' have values '.csv' and 'example_results' respectively.
- **Edit Button**: the <div className="button dark_gray_button">Edit</div> button that unlocks the _node_ for modifying _node_ name, changing _node_ attributes, or deleting the _node_

Notebooks that back nodes are laid out in the following order:

- **User-defined SQL** for retrieving tabular data to be processed by the node
- **User-defined Python** for processing input
- **Save Pipeline Code** for saving and deploying user-defined SQL and Python snippets to the workflow management system, enabling users to run flows
- **Testing Section** for testing changes to user-defined SQL and Python

A full listing of available nodes and their key characteristics can be found on the [Node Overview](./nodes/NodeOverview) page.

## Backing Notebooks

<div style={{textAlign: 'center'}}>
  <img alt="Ganymede notebook"
      src="https://ganymede-bio.mo.cloudinary.net/apiServer/GanymedeNotebook_20230531.png" />
</div>


Editable nodes contain a pencil icon (<EditOutlined className="button darkblue_button"/>) in their upper right hand corner that opens up the notebook backing the node upon button click.  This section walks through the code associated with such a node to discuss its functionality.

### Notebook: User-defined SQL

```python
from ganymede_sdk.editor import query

query_sql = """
  SELECT * FROM demo_file;
"""
```

For nodes that reference the Ganymede data lake on input, the _query_sql_ string is used to specify the data table to process on the input parameter in the _execute_ function.  The schema in the query is specific to the tenant used, and the file corresponds to the name used in an upstream node.

:::info observing columns and tables in Ganymede data lake

Available tables and their associated schemas can be observed in the [Data Explorer](./DataExplorer.mdx) tab of the Ganymede app.

:::

More than 1 query can be specified as input; to do so, specify multiple queries as semicolon-delimited query strings.  These are passed as a parameter to the user-defined Python function as a list of tables.


### Notebook: User-defined Python

```python
import pandas as pd
from typing import Union, Dict, List

def execute(df_sql_result: Union[pd.DataFrame, List[pd.DataFrame]], ganymede_context=None) -> Union[pd.DataFrame, Dict[str, pd.DataFrame]]:
    """
    Process dataframe(s) resulting from SQL query using python

    Parameters
    ----------
    df_sql_result: DataFrame or List[DataFrame]
      Data input from SQL output of query_str string

    ganymede_context: GanymedeContext (optional)
      Context variable for flow runs; contains input file name(s) and flow run timestamp.

    Returns
    -------
    DataFrame or Dict[str, DataFrame]
      If a pandas DataFrame is returned, the table name that the data is stored to within the Ganymede data lake corresponds to the 'analysis' value on the node.

      If a dictionary is returned, keys are table names in the Ganymede data lake.  The DataFrame with the keyword 'analysis' as the key will be saved to the Ganymede data lake as specified by the 'analysis' variable on the node.
    """

    if isinstance(df_sql_result, pd.DataFrame):
        df_out = df_sql_result.copy()
    else:
        df_out = {k: df for k, df in enumerate(df_sql_result)}

    return df_out
```

The _execute_ function can be viewed analogous to the _main_ function in Python; it is the starting point for execution by the workflow manager when user-defined Python is executed.

In this example, the _execute_ function takes the results of 1 or more tables as input, presented as either a pandas DataFrame or list of pandas DataFrames.  The execute function returns either a pandas DataFrame or a dictionary of pandas DataFrames with table names as keys.

By default, if only 1 DataFrame is returned, it would be displayed in the table head associated with the node.  If multiple DataFrames are returned, the table displayed corresponds to table name specified on the node parameter.  In the example above, the dataframe correponding to **sql_result** key would be the one displayed if multiple tables were returned from the _execute_ function.


:::caution

The names of the _query_sql_ variable and _execute_ function cannot be modified or moved to different cells in a notebook.  Upon running the save cell in the notebook, the cells that contain the _query_sql_ string and execute function are saved, and the contents of all other cells are discarded.  

This allows users to conduct testing in other cells prior to committing and loading code to the workflow orchestrator.

:::

### Notebook: Save Pipeline Code cell

```python
# Save the updated query and python function to the pipeline
from ganymede_sdk.editor import save

files = { 'query.sql': query_sql }
save(files)
```

Running this cell commits any code changes to the Git repository associated with the tenant and uploads the code to the workflow management system for executing code.

### Notebook: Testing Section

```python
# Example 1: Testing section for Transform_Py node

from ganymede_sdk.io import retrieve_sql
from ganymede_sdk.editor import MockGanymedeContext

# Retrieve Ganymede context object from prior run, to mimic the execution environment of most recent pipeline run
mock_ganymede_context = MockGanymedeContext()

# Retrieve dataframes to process; the context object enables rendering of Jinja template variables used in data retrieval
# More detail can be found on the Flow Context & Metadata page of this documentation, under the Template Variables section
result_dfs = retrieve_sql(query_sql, context=mock_ganymede_context)
result_dfs

# Run user-defined python on retrieved data
execute(result_dfs, ganymede_context=mock_ganymede_context)
```

The testing section enables users to test pipeline code prior to deployment.  In the example above, the testing code executes _query_sql_ and sends the result to the _execute_ function.  

_Nodes_ that load data use a file upload widget so that a file can be uploaded prior to processing.  Running the code below yields a button that allows users to submit a CSV file.  Data associated with the code can be accessed by the execute function as shown in the 2nd code snippet below.

```python
# Example 2: Testing section for Read_CSV node

# Create widget for uploading CSV for testing node operation
from IPython.display import display
from io import BytesIO
import ipywidgets as widgets

file_uploader = widgets.FileUpload(
    accept='.csv',
    multiple=False,
)
display(file_uploader)

# Retrieve context variable from most recent run
from ganymede_sdk.editor import MockGanymedeContext
mock_ganymede_context = MockGanymedeContext()

# Run user-defined Python on uploaded file
df = execute({'test_file': BytesIO(file_uploader.value[0].content.tobytes())})
display(df)
```

### Notebook Tips

Functions can be introspected by typing ? or ?? before the function name in a notebook cell.  For example:

#### Tip 1: Reading function signature and docstring

```python
from ganymede_sdk.io import retrieve_sql

# shows function signature and docstring 
?retrieve_sql
```

<img alt="Explore documentation" 
      src="https://ganymede-bio.mo.cloudinary.net/apiServer/Example_Function_Docstring20230712.png"
      />
&nbsp;

#### Tip 2: Reading function signature and source code

```python
# shows function signature and source code (image below truncated for brevity)
??retrieve_sql
```

<img alt="Explore source code" 
      src="https://ganymede-bio.mo.cloudinary.net/apiServer/Example_Function_Source_Narrow20230712.png"
      />
&nbsp;

#### Tip 3: Tab completion for object exploration

Pressing <TAB\> key enables exploration of objects, modules, and namespaces.

```python
from ganymede_sdk.editor import MockGanymedeContext
mock_ganymede_context = MockGanymedeContext()
mock_ganymede_context.<TAB>
```


<img alt="Observe object methods" 
      src="https://ganymede-bio.mo.cloudinary.net/apiServer/Example_Tab_Completion20230712.png"
      width="600"
      />


## Adding flow inputs

_Flows_ can be configured to accept user inputs at the start of each run by selecting the appropriate node and saving the _flow_. There are three types of inputs:

- File inputs (e.g. - FileAny, FileCSV, etc.)
- Text (string) inputs (e.g. - string)
- Tag inputs (e.g. - TagBenchling)

A list of available _nodes_ and their associated categories can be found on the [table listing of node characteristics](./nodes/Overview.mdx#table-listing-of-node-characteristics)  More detail can be found on the [Node documentation page](./nodes/Overview.mdx).

### Table Heads

<div class="text--center">
<img width="400" alt="image" src="https://ganymede-bio.mo.cloudinary.net/apiServer/TableHead_20230522.png"/>
</div>

The _table head_ exists for _nodes_ that produce an output table.  For this subset of _nodes_, the _table head_ shows 5 records of the primary table produced by its associated _node_.  This table can facilitate the development of downstream _flow_ components.

